{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, './text2num/')\n",
    "import text2num\n",
    "from dateutil import parser\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"ufo_nosummary.pkl\",\"r\") as pf:\n",
    "    d = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Make into pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([d[\"city\"],d[\"title\"],d[\"shape\"],d[\"duration\"],d[\"state\"],d[\"link\"],d[\"date\"]])\n",
    "df = df.transpose()\n",
    "df.columns = ['city','title','shape','duration','state','link','date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shapes(x):\n",
    "    dic = {\"triangular\":\"triangle\",\"triange\":\"triangle\",\"round\":\"circle\",\n",
    "           \"changed\":\"changing\",\"pyramid\":\"triangle\",\"flare\":\"light\",\n",
    "           \"dome\":\"disk\",\"hexagon\":\"other\",\"crescent\":\"other\",\"delta\":\"chevron\"}\n",
    "    if x in dic.keys():\n",
    "        return dic[x]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['shape'] = df['shape'].map(lambda x: shapes(x.lower()) if x else \"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_dt(dateStr):\n",
    "    for fmt in [\"%m/%d/%y\",\"%m/%d/%y %H:%M\"]:\n",
    "        try:\n",
    "            return datetime.strptime(dateStr, fmt)\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not using\n",
    "\n",
    "def date_only(x):\n",
    "    reg_date = \"\\d{1,2}\\/\\d{1,2}\\/\\d{2}\"\n",
    "    result = re.findall(reg_date,str(x))\n",
    "    if len(result)>0:\n",
    "        return datetime.strptime(result[0], \"%m/%d/%y\")\n",
    "    else:\n",
    "        return \"None\"\n",
    "def time_only(x):\n",
    "    reg_time = \"\\d{2}\\:\\d{2}\"\n",
    "    result = re.findall(reg_time,str(x))\n",
    "    if len(result)>0:\n",
    "        return datetime.strptime(result[0], \"%H:%M\")\n",
    "    else:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['date_datetime'] = df['date'].map(lambda x: standardize_dt(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['day']=df['date_datetime'].map(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertTime(elem):  \n",
    "    t = 1\n",
    "    lower = elem.lower()\n",
    "    #TODO: learn regex\n",
    "    split = elem.replace(\"-\",\" \").replace(\"s\",\" s\").replace(\"h\",\" h\").replace(\"m\",\" m\").replace(\":00\",\" min \").replace(\"00:\",\" \").replace(\"~\",\" \").replace(\"_\",\" \").replace(\"+\",\" \").replace(\"<\",\" \").replace(\">\",\" \").split(\" \")\n",
    "    num = np.mean([float(s) for s in split if s.replace(\".\", \"\",1).isdigit()])\n",
    "    if \"minutes\" in lower or \"minute\" in lower or \"min\" in lower or \"mins\" in lower or \"m \" in lower:\n",
    "    #if \"m \" in split:\n",
    "        t = 60\n",
    "    elif \"hours\" in lower or \"hour\" in lower or \"hrs\" in lower or \"hr\" in lower or \"h \" in lower:\n",
    "    #elif \"h \" in split:\n",
    "        t = 3600\n",
    "    #times.append(num*t)\n",
    "    if np.isnan(num)==False:\n",
    "        return num*t\n",
    "    if np.isnan(num)==True:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#twice as fast as my code :(\n",
    "def infer_duration_in_seconds(text):\n",
    "    # try different regexps to extract the total seconds\n",
    "    metric_text = [\"hour\",\"minute\",\"second\",\"sec\",\"segundo\",\"min\",\"hr\",\" m\",\" h\",\" s\"]\n",
    "    metric_seconds = [3600,60,1,1,1,60,3600,60,3600,1]\n",
    "    text_replaced = text.replace(\":\",\" min \").replace(\"+\",\"\").replace(\"-\",\" \").lower() #check for colon\n",
    "    \n",
    "    #digit and metric together\n",
    "    for metric,mult in zip(metric_text,metric_seconds):\n",
    "        regex = \"\\s*(\\d+)\\+?\\s*{}s?\".format(metric)\n",
    "        res = re.findall(regex,text_replaced)\n",
    "        if len(res)>0:\n",
    "            return int(float(res[0]) * mult)\n",
    "        \n",
    "    #only metric\n",
    "    for metric,mult in zip(metric_text,metric_seconds):\n",
    "        metric_regex = \"\\s*\\+?\\s*{}\\s?\".format(metric)\n",
    "        res = re.findall(metric_regex,text_replaced)\n",
    "        if len(res)>0:\n",
    "            return mult\n",
    "    \n",
    "    #only digit (seconds)\n",
    "    dig_regex = \"\\s*(\\d+)\\+?\\s*?\"\n",
    "    res = re.findall(dig_regex,text_replaced)\n",
    "    if len(res)>0:\n",
    "        return int(float(res[0]) * 1)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.14482498169\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "df['duration_seconds'] = df['duration'].map(lambda x: infer_duration_in_seconds(x))\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   0\n",
       "title                  0\n",
       "shape                  0\n",
       "duration               0\n",
       "state                  0\n",
       "link                   0\n",
       "date                   0\n",
       "date_datetime        242\n",
       "duration_seconds    6306\n",
       "geo                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Link col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['link_no_ext'] = df['link'].map(lambda x: x[:-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read all geocode files (where I geocoded the addresses not initially coded) and appended to one df\n",
    "files = [\"1000_2000\",\"2000_3000\",\"3000_4000\",\"4000_5000\",\"5000_6000\",\\\n",
    "         \"6000_7000\",\"7000_8000\",\"8000_9000\",\"9000_10000\",\"10000_12000\"]\n",
    "\n",
    "#initialize df\n",
    "geo_df_full = pd.read_csv(\"0_1000_geocoded.csv\",header=0)\n",
    "del geo_df_full['Unnamed: 0']\n",
    "geo_df_full.drop_duplicates(subset=['FullLocation'],inplace=True)\n",
    "geo_df_full = geo_df_full.reset_index(drop=True)\n",
    "\n",
    "#iterate through other geocode files\n",
    "for f in files:\n",
    "    geo_df = pd.read_csv(\"{}_geocoded.csv\".format(f),header=False)\n",
    "    del geo_df['Unnamed: 0']\n",
    "    geo_df = geo_df.reset_index(drop=True)\n",
    "\n",
    "    geo_df_full= geo_df_full.append(geo_df,ignore_index=True)\n",
    "\n",
    "geo_df_full.drop_duplicates(subset=['FullLocation'],inplace=True)\n",
    "del geo_df_full['Unnamed: 0.1']\n",
    "del geo_df_full['FullLocation']\n",
    "\n",
    "#filter out 0's for lat/long\n",
    "geo_df_full = geo_df_full[geo_df_full['lon']!=0.0]\n",
    "geo_df_full = geo_df_full[geo_df_full['lat']!=0.0]\n",
    "\n",
    "#force column names to match previous df\n",
    "geo_df_full.columns = ['city','state','long','lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#full df\n",
    "df = pd.read_csv(\"ufo_nosummary.csv\")\n",
    "del df[\"Unnamed: 0\"]\n",
    "\n",
    "#unique geocodes\n",
    "df_geocodes = pd.read_csv(\"geocodes_8973.csv\")\n",
    "del df_geocodes['Unnamed: 0']\n",
    "df_geocodes.drop_duplicates(subset=['city','state'],inplace=True)\n",
    "df_geocodes = df_geocodes.reset_index(drop=True)\n",
    "\n",
    "#split lat/long\n",
    "df_test = pd.DataFrame(df_geocodes.latlong.str.split(',',1).tolist(),\n",
    "                                   columns = ['lat','long'])\n",
    "df_test['lat'] = df_test['lat'].map(lambda x: x[1:])\n",
    "df_test['long'] = df_test['long'].map(lambda x: x[:-1])\n",
    "del df_geocodes['latlong']\n",
    "\n",
    "#unique geocodes with lat/long separated\n",
    "df_5359_unique = pd.concat([df_geocodes,df_test],axis=1)\n",
    "del df_5359_unique['link']\n",
    "\n",
    "#merge with new geocodes (defined above)\n",
    "all_the_geocodes = pd.merge(df_5359_unique,geo_df_full,how='outer')\n",
    "\n",
    "#full df merged with unique geocodes, geocodes 56476!\n",
    "df_geocoded = pd.merge(df,all_the_geocodes,how='inner',on=['city','state'])\n",
    "del df_geocoded['geo']\n",
    "del df_geocoded['title']\n",
    "df_geocoded = df_geocoded[df_geocoded['date_datetime'].isnull()==False]\n",
    "\n",
    "#full df merged with unique geocodes, geocodes includes NaNs\n",
    "df_geocoded_full = pd.merge(df,all_the_geocodes,how='outer',on=['city','state'])\n",
    "del df_geocoded_full['geo']\n",
    "del df_geocoded_full['title']\n",
    "df_geocoded_full = df_geocoded_full[df_geocoded_full['date_datetime'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_geocoded['pre2015'] = df_geocoded['date_datetime'].map(lambda x: parser.parse(x).year < 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     82604\n",
       "False     1287\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geocoded.pre2015.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83891"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_geocoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(df_geocoded['date_datetime'][987]).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_geocoded['daylight'] = df_geocoded['date_datetime'].map(lambda x: parser.parse(x).hour>9 and parser.parse(x).hour<18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    71670\n",
       "True     12221\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geocoded['daylight'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_geocoded['USA'] = df_geocoded['state'].map(lambda x: True if x in states else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     78174\n",
       "False     5717\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geocoded['USA'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###See df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_geocoded.to_csv(\"83891_locations.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99477"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full\n",
    "len(df_56476_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56476"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#geocoded\n",
    "len(df_56476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19120"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not geocoded\n",
    "#where both city and state are complete\n",
    "len(not_yet_geocoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23155"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_states_notgeo = df_56476_full[df_56476_full['lat'].isnull()][['city','state']].drop_duplicates()\n",
    "len(city_states_notgeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_yet_geocoded = city_states_notgeo[city_states_notgeo['city'].isnull()==False].reset_index(drop=True)\n",
    "not_yet_geocoded = not_yet_geocoded[not_yet_geocoded['state'].isnull()==False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_yet_geocoded.to_csv(\"19120_not_geocoded.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###States dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
